# Experiment: Baseline-Konfiguration für Temporal Fusion Transformer (TFT)
# Ziel: stabile, reproduzierbare Referenz für spätere Hyperparameter-Studien
# Datum: 2025-11-09
# Version: v01_baseline

# Reproduzierbarkeit
seed: 42
accelerator: "cpu"        # bei GPU: "gpu"
devices: 1

# ----------------------------
# Trainer-Konfiguration
# ----------------------------
max_epochs: 5            # genug, um Lernkurve zu stabilisieren
batch_size: 128           # mittlere Größe, gute Balance aus Geschwindigkeit & Stabilität
learning_rate: 0.0007     # ANPASSUNG (Baseline: 0.001)
gradient_clip_val: 0.1    # verhindert Explodieren der Gradienten
early_stopping_patience: 5
num_workers: 4            # Datensatz-Parallelisierung
limit_train_batches: 1.0  # nutzt gesamten Trainingssatz
limit_val_batches: 1.0

# ----------------------------
# Modellparameter (TFT)
# ----------------------------
model:
  loss: "quantile"            # verwendet QuantileLoss für probabilistische Vorhersagen
  hidden_size: 16             # moderate Modellgröße
  attention_head_size: 4
  dropout: 0.1
  hidden_continuous_size: 8
  output_size: 7              # 7 Quantile (z. B. [0.1, 0.2, ..., 0.9])
  reduce_on_plateau_patience: 2
