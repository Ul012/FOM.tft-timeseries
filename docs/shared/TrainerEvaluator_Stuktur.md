# Struktur und Verantwortlichkeiten: Trainer ‚Äì Evaluator ‚Äì Visualizer

**Projektkontext:** *TFT-TimeSeries*  
Ziel ist eine saubere Trennung von Verantwortlichkeiten zwischen den Komponenten des Projekts, um die Nachvollziehbarkeit, Erweiterbarkeit und Wartbarkeit zu sichern.

---

## üéØ Grundprinzipien

- Jede Komponente (Trainer, Evaluator, Visualizer) erf√ºllt **eine klar abgegrenzte Aufgabe**.  
- Jede Komponente schreibt ihre Ergebnisse in **eindeutig definierte Ordner**.  
- Kein Skript √ºberschreibt oder dupliziert Zust√§ndigkeiten anderer Module.  

Diese Struktur folgt dem Prinzip der **Separation of Concerns** und dem **Single Responsibility Principle (SRP)**.

---

## ‚öôÔ∏è Komponenten√ºbersicht

| Komponente | Hauptaufgabe | Typische Datei | Verantwortlich f√ºr |
|-------------|---------------|----------------|--------------------|
| **Trainer** | Modell trainieren und Laufzeitmetriken erfassen | `src/modeling/trainer_tft.py` | Training, Logging, Checkpoints |
| **Evaluator** | Ergebnisse bewerten und zusammenfassen | `src/evaluation/evaluate_tft.py` | Berechnung und Aggregation der Metriken |
| **Visualizer** | Ergebnisse grafisch darstellen | `src/visualization/compare_runs.py` / `plot_learning_rate.py` | Plots, Reports, Trends |

---

## üß© 1. Trainer

**Aufgabe:**  
F√ºhrt das Modelltraining aus, protokolliert Metriken und speichert alle laufzeitbezogenen Artefakte.

**Verantwortlichkeiten:**
- Initialisierung des Modells (`TemporalFusionTransformer.from_dataset(...)`)
- Definition von Verlust- und Bewertungsmetriken (`QuantileLoss`, `MAE`, `RMSE`, `MAPE`, `SMAPE`)
- Logging (Loss, Metriken, Lernrate)
- Speichern der Checkpoints
- Ausgabe eines `metrics.csv`-Logs

**Ordnerstruktur:**
```
logs/
‚îî‚îÄ tft/
   ‚îî‚îÄ run_YYYYMMDD_HHMMSS/
      ‚îú‚îÄ metrics.csv              # Laufzeitmetriken (train/val/lr)
      ‚îú‚îÄ checkpoints/             # Beste Gewichte (ModelCheckpoint)
      ‚îú‚îÄ hparams.yaml             # Hyperparameter pro Run
      ‚îî‚îÄ run_summary.csv          # Letzte Epoche zusammengefasst
```

**Ausgabe:**  
- `metrics.csv` ‚Üí vollst√§ndige Metrikverl√§ufe (Loss, MAE, RMSE, LR)  
- `run_summary.csv` ‚Üí letzte Epoche als Kurzreport  
- `checkpoints/` ‚Üí gespeicherte Modellgewichte  

---

## üßÆ 2. Evaluator

**Aufgabe:**  
Analysiert abgeschlossene Trainingsl√§ufe und erzeugt standardisierte Auswertungen.  

**Verantwortlichkeiten:**
- Lesen aller `metrics.csv` aus `logs/tft/run_*`
- Ermitteln der finalen Werte (z.‚ÄØB. letzte Zeile pro Run)
- Erstellen von Vergleichstabellen (`runs_summary.csv`)
- Optional: Berechnung zus√§tzlicher Kennzahlen aus gespeicherten Checkpoints

**Ordnerstruktur:**
```
results/
‚îî‚îÄ evaluation/
   ‚îú‚îÄ runs_summary.csv        # Kompakte Vergleichstabelle √ºber alle Runs
   ‚îú‚îÄ eval_metrics.csv        # Ergebnisse aus geladenen Checkpoints (optional)
   ‚îî‚îÄ reports/                # sp√§tere Text-/PDF-Berichte
```

**Ausgabe:**  
- `results/evaluation/runs_summary.csv` ‚Üí konsolidierte √úbersicht √ºber alle Runs  
- (optional) `results/evaluation/eval_metrics.csv` ‚Üí nachtr√§glich berechnete Kennzahlen

---

## üìä 3. Visualizer

**Aufgabe:**  
Erstellt aus Logs und Evaluationsergebnissen anschauliche Darstellungen (Lernkurven, Run-Vergleiche).

**Verantwortlichkeiten:**
- Plotten von Loss- und LR-Verl√§ufen aus `metrics.csv`
- Plotten von Balkendiagrammen aus `runs_summary.csv`
- Speichern der Visualisierungen im Unterordner `results/plots/`

**Ordnerstruktur:**
```
results/
‚îî‚îÄ plots/
   ‚îú‚îÄ learning_curve_with_params_bottom.png
   ‚îú‚îÄ runs_comparison.png
   ‚îî‚îÄ weitere Visualisierungen
```

**Ausgabe:**  
- Diagramme, die direkt aus CSV-Dateien erzeugt werden  
- Bereitstellung f√ºr Dokumentation (z.‚ÄØB. MkDocs oder README)

---

## üß† 4. Gesamtfluss (Trainer ‚Üí Evaluator ‚Üí Visualizer)

```
(1) Trainer
     ‚îÇ
     ‚îú‚îÄ‚îÄ trainiert Modell
     ‚îú‚îÄ‚îÄ schreibt logs/tft/run_*/metrics.csv
     ‚îî‚îÄ‚îÄ speichert Checkpoints
            ‚Üì
(2) Evaluator
     ‚îÇ
     ‚îú‚îÄ‚îÄ liest alle metrics.csv
     ‚îú‚îÄ‚îÄ extrahiert finale Metriken
     ‚îî‚îÄ‚îÄ schreibt results/evaluation/runs_summary.csv
            ‚Üì
(3) Visualizer
     ‚îÇ
     ‚îú‚îÄ‚îÄ liest runs_summary.csv & metrics.csv
     ‚îú‚îÄ‚îÄ erstellt Plots
     ‚îî‚îÄ‚îÄ speichert in results/plots/
```

---

## üß≠ 5. Zuordnung der Dateitypen

| Dateityp | Inhalt | Herkunft | Ablage |
|-----------|---------|-----------|---------|
| `.csv` | Metriken, Tabellen | Trainer, Evaluator | `logs/`, `results/` |
| `.ckpt` | Modellgewichte | Trainer | `logs/.../checkpoints/` |
| `.yaml` | Hyperparameter | Trainer | `logs/.../` |
| `.png` | Visualisierungen | Visualizer | `results/plots/` |
| `.md` | Dokumentation | Manuell / MkDocs | `docs/`, `notes/` |

---

## ‚úÖ 6. Vorteile der Trennung

| Vorteil | Beschreibung |
|----------|---------------|
| **Klarheit** | Jede Komponente hat einen klaren Verantwortungsbereich |
| **Wartbarkeit** | √Ñnderungen an Training oder Evaluation erfordern keine Eingriffe in andere Module |
| **Reproduzierbarkeit** | Jeder Run ist durch `logs/run_*` vollst√§ndig nachvollziehbar |
| **Automatisierung** | Pipeline-Skripte k√∂nnen gezielt einzelne Phasen (train, eval, visualize) ansto√üen |
| **Erweiterbarkeit** | Weitere Modelle oder Evaluatoren k√∂nnen einfach erg√§nzt werden |

---

## üìö 7. N√§chste Schritte

1. Sicherstellen, dass `LearningRateMonitor` im Trainer aktiv ist.  
2. `evaluate_tft.py` erweitern, um finale Metriken automatisch aus allen Runs zu extrahieren.  
3. Erste Visualisierung mit `runs_summary.csv` aufbauen.  
4. Alle Ergebnisse im `results/`-Ordner dokumentieren.

---

**Kurzfazit:**  
> Der **Trainer** produziert Laufzeitdaten (logs),  
> der **Evaluator** verdichtet diese zu aussagekr√§ftigen Ergebnissen (results),  
> und der **Visualizer** macht sie sichtbar und verst√§ndlich.
